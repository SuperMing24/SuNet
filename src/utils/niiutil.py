import nibabel as nib
import numpy as np
import torch
from utils.segutil import SegDataset
from typing import Dict, List, Tuple, Any, Optional
import matplotlib
matplotlib.use('Agg')  # åœ¨å¯¼å…¥ pyplot ä¹‹å‰è®¾ç½® Matplotlib ä½¿ç”¨ Agg åç«¯
import matplotlib.pyplot as plt
plt.ioff()  # å…³é—­äº¤äº’æ¨¡å¼
import json
import os


class DataInspector:
    """
    æ•°æ®æ£€æŸ¥å™¨ï¼šè´Ÿè´£æ•°æ®çš„ç»Ÿè®¡åˆ†æå’Œæ£€æŸ¥
    """

    @staticmethod
    def get_shape_info(data: np.ndarray, name: str = "data") -> Dict[str, Any]:
        """è·å–å½¢çŠ¶ä¿¡æ¯çš„ç»Ÿä¸€æ–¹æ³•"""
        return {
            f"{name}_shape": data.shape,
            f"{name}_ndim": data.ndim,
            f"{name}_dtype": str(data.dtype),
            f"{name}_size": data.size
        }

    @staticmethod
    def get_image_statistical(data: np.ndarray, name: str = "image") -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯çš„ç»Ÿä¸€æ–¹æ³•"""
        return {
            f"{name}_min": float(data.min()),
            f"{name}_max": float(data.max()),
            f"{name}_mean": float(data.mean()),
            f"{name}_std": float(data.std()),
            f"{name}_median": float(np.median(data)),
            f"{name}_has_nan": np.isnan(data).any(),
            f"{name}_has_inf": np.isinf(data).any()
        }

    @staticmethod
    def get_mask_distribution(mask: np.ndarray, name: str = "mask") -> Dict[str, Any]:
        """è·å–æ©ç åˆ†å¸ƒä¿¡æ¯"""
        unique, counts = np.unique(mask, return_counts=True)
        total_pixels = mask.size

        distribution = {}
        for val, count in zip(unique, counts):
            percentage = (count / total_pixels) * 100
            distribution[f"{name}_class_{int(val)}_pixels"] = int(count)
            distribution[f"{name}_class_{int(val)}_percentage"] = float(percentage)

        return distribution

    @staticmethod
    def visualize_sample(image: np.ndarray, mask: np.ndarray, save_path: Optional[str] = None):
        """å¯è§†åŒ–æ ·æœ¬æ•°æ®"""
        fig, axes = plt.subplots(1, 3, figsize=(15, 5))

        # æ˜¾ç¤ºå›¾åƒ
        if image.ndim == 3 and image.shape[2] in [1, 3]:
            display_image = image if image.shape[2] == 3 else image[:, :, 0]
            axes[0].imshow(display_image, cmap='gray')
        else:
            axes[0].imshow(image, cmap='gray')
        axes[0].set_title('è¾“å…¥å›¾åƒ')
        axes[0].axis('off')

        # æ˜¾ç¤ºæ©ç 
        axes[1].imshow(mask.squeeze(), cmap='jet')
        axes[1].set_title('çœŸå®æ©ç ')
        axes[1].axis('off')

        # æ˜¾ç¤ºå åŠ æ•ˆæœ
        axes[2].imshow(display_image, cmap='gray')
        axes[2].imshow(mask.squeeze(), cmap='jet', alpha=0.5)
        axes[2].set_title('å›¾åƒ+æ©ç å åŠ ')
        axes[2].axis('off')

        plt.tight_layout()

        if save_path:
            os.makedirs(os.path.dirname(save_path), exist_ok=True)
            plt.savefig(save_path, dpi=150, bbox_inches='tight')
            print(f"ğŸ“Š æ ·æœ¬å¯è§†åŒ–å·²ä¿å­˜: {save_path}")

        plt.close()


class DataReporter:
    """æ•°æ®æŠ¥å‘Šç”Ÿæˆå™¨ - è´Ÿè´£ç»“æ„åŒ–è¾“å‡ºæ£€æŸ¥ç»“æœ"""

    @staticmethod
    def print_complete_report(dataset_stats: Dict[str, Any], show_individual_samples = False):
        """æ‰“å°æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯"""
        if not dataset_stats:
            print("âŒ æ²¡æœ‰å¯ç”¨çš„ç»Ÿè®¡æ•°æ®")
            return

        print("\n" + "=" * 80)
        print("ğŸ“Š NIfTI æ•°æ®é›†æ£€æŸ¥æŠ¥å‘Š")
        print("=" * 80)

        # 1. æ•´ä½“æ¦‚è§ˆ
        DataReporter.print_overview(dataset_stats)

        # 2. æ–‡ä»¶çº§åˆ«ç»Ÿè®¡æ‘˜è¦
        DataReporter.print_file_stats(dataset_stats)

        DataReporter.print_anomaly_check(dataset_stats)

        # 3. æ€»ä½“æ ·æœ¬ç»Ÿè®¡
        DataReporter.print_statistical_summary(dataset_stats)

        # 4. å•ä¸ªæ ·æœ¬ç»Ÿè®¡
        if show_individual_samples:
            DataReporter._print_individual_sample_stats(dataset_stats)

        print("\n" + "=" * 80)
        print("ğŸ‰ æ•°æ®æ£€æŸ¥æŠ¥å‘Šç”Ÿæˆå®Œæˆ!")
        print("=" * 80)


    @staticmethod
    def print_overview(dataset_stats: Dict[str, Any]):
        # æ–‡ä»¶åŸºæœ¬ä¿¡æ¯æ¦‚è§ˆ
        total_files = dataset_stats.get('total_files', 0)
        total_samples = dataset_stats.get('total_samples', 0)

        print(f"\nğŸ“ æ•°æ®é›†åŸºæœ¬ä¿¡æ¯:")
        print(f"  â”œâ”€â”€ æ–‡ä»¶å¯¹æ•°é‡: {total_files}")
        print(f"  â”œâ”€â”€ æ€»æ ·æœ¬æ•°: {total_samples}")

        if total_files > 0 and total_samples > 0:
            avg_samples_per_file = total_samples / total_files
            print(f"  â””â”€â”€ å¹³å‡æ¯æ–‡ä»¶æ ·æœ¬æ•°: {avg_samples_per_file:.1f}")

    @staticmethod
    def print_file_stats(dataset_stats: Dict[str, Any], max_files: int = 10):
        """æ‰“å°æ–‡ä»¶çº§åˆ«ç»Ÿè®¡ä¿¡æ¯"""
        file_pairs = dataset_stats.get('file_pairs', [])
        if not file_pairs:
            return

        print(f"\nğŸ“„ æ–‡ä»¶ç»Ÿè®¡ (æ˜¾ç¤ºå‰ {min(max_files, len(file_pairs))} ä¸ªæ–‡ä»¶):")
        print("-" * 60)

        for i, file_info in enumerate(file_pairs[:max_files]):
            print(f"\nğŸ”¹ æ–‡ä»¶å¯¹ {i + 1}:")
            print(f"   å›¾åƒæ–‡ä»¶: {os.path.basename(file_info['image_file'])}")
            print(f"   æ©ç æ–‡ä»¶: {os.path.basename(file_info['mask_file'])}")

            # å½¢çŠ¶ä¿¡æ¯
            image_shape = file_info['image_shape']
            mask_shape = file_info['mask_shape']
            print(f"   åŸå§‹å›¾åƒå½¢çŠ¶: {image_shape['åŸå§‹å›¾åƒ_shape']}")
            print(f"   åŸå§‹æ©ç å½¢çŠ¶: {mask_shape['åŸå§‹æ©ç _shape']}")

    @staticmethod
    def print_statistical_summary(dataset_stats: Dict[str, Any]):
        """æ‰“å°ç»Ÿè®¡æ‘˜è¦"""
        file_pairs = dataset_stats.get('file_pairs', [])
        if not file_pairs:
            return

        print(f"\nğŸ“ˆ ç»Ÿè®¡æ‘˜è¦ (åŸºäº {len(file_pairs)} ä¸ªæ–‡ä»¶):")
        print("-" * 60)

        # æ”¶é›†æ‰€æœ‰ç»Ÿè®¡ä¿¡æ¯
        all_raw_image_stats = []
        all_processed_image_stats = []
        all_raw_mask_stats = []
        all_processed_mask_stats = []

        for file_info in file_pairs:
            all_raw_image_stats.append(file_info['image_raw_stats'])
            all_processed_image_stats.append(file_info['image_processed_stats'])
            all_raw_mask_stats.append(file_info['mask_raw_stats'])
            all_processed_mask_stats.append(file_info['mask_processed_stats'])

        # å›¾åƒç»Ÿè®¡
        print("\nğŸ¯ å›¾åƒæ•°å€¼ç»Ÿè®¡:")
        DataReporter._print_image_statistical(
            all_raw_image_stats, all_processed_image_stats, "å›¾åƒ"
        )

        # æ©ç ç»Ÿè®¡
        print("\nğŸ¯ æ©ç åˆ†å¸ƒç»Ÿè®¡:")
        DataReporter._print_mask_statical(all_raw_mask_stats, all_processed_mask_stats)

    @staticmethod
    def _print_image_statistical(raw_image_stats_list: List[Dict], processed_image_stats_list: List[Dict], data_type: str):
        """æ‰“å°ç»Ÿè®¡å¯¹æ¯”ä¿¡æ¯"""
        if not raw_image_stats_list or not processed_image_stats_list:
            return

        # åŸå§‹æ•°æ®ç»Ÿè®¡
        raw_means = [stats[f'åŸå§‹{data_type}_mean'] for stats in raw_image_stats_list]
        raw_stds = [stats[f'åŸå§‹{data_type}_std'] for stats in raw_image_stats_list]
        raw_mins = [stats[f'åŸå§‹{data_type}_min'] for stats in raw_image_stats_list]
        raw_maxs = [stats[f'åŸå§‹{data_type}_max'] for stats in raw_image_stats_list]

        # å¤„ç†åæ•°æ®ç»Ÿè®¡
        processed_means = [stats[f'å½’ä¸€åŒ–å{data_type}_mean'] for stats in processed_image_stats_list]
        processed_stds = [stats[f'å½’ä¸€åŒ–å{data_type}_std'] for stats in processed_image_stats_list]
        processed_mins = [stats[f'å½’ä¸€åŒ–å{data_type}_min'] for stats in processed_image_stats_list]
        processed_maxs = [stats[f'å½’ä¸€åŒ–å{data_type}_max'] for stats in processed_image_stats_list]

        print(f"  åŸå§‹æ•°æ®:")
        print(f"    â”œâ”€â”€ å‡å€¼èŒƒå›´: [{min(raw_means):.4f}, {max(raw_means):.4f}]")
        print(f"    â”œâ”€â”€ æ ‡å‡†å·®èŒƒå›´: [{min(raw_stds):.4f}, {max(raw_stds):.4f}]")
        print(f"    â”œâ”€â”€ æœ€å°å€¼èŒƒå›´: [{min(raw_mins):.4f}, {max(raw_mins):.4f}]")
        print(f"    â””â”€â”€ æœ€å¤§å€¼èŒƒå›´: [{min(raw_maxs):.4f}, {max(raw_maxs):.4f}]")

        print(f"  å½’ä¸€åŒ–å:")
        print(f"    â”œâ”€â”€ å‡å€¼èŒƒå›´: [{min(processed_means):.4f}, {max(processed_means):.4f}]")
        print(f"    â”œâ”€â”€ æ ‡å‡†å·®èŒƒå›´: [{min(processed_stds):.4f}, {max(processed_stds):.4f}]")
        print(f"    â”œâ”€â”€ æœ€å°å€¼èŒƒå›´: [{min(processed_mins):.4f}, {max(processed_mins):.4f}]")
        print(f"    â””â”€â”€ æœ€å¤§å€¼èŒƒå›´: [{min(processed_maxs):.4f}, {max(processed_maxs):.4f}]")

        print(f"  æ€»å¹³å‡é‡ä¸å˜åŒ–è¶‹åŠ¿ï¼š")
        print(f"    â”œâ”€â”€ å‡å€¼: [{np.mean(raw_means)} â€”â€”> {np.mean(processed_means)}] "
              f"{'â†“å‡å°' if np.mean(processed_means) < np.mean(raw_means) else 'â†‘å¢å¤§'}")
        print(f"    â”œâ”€â”€ æ ‡å‡†å·®: [{np.mean(raw_stds)} â€”â€”> {np.mean(processed_stds)}] "
              f"{'â†“å‡å°' if np.mean(processed_stds) < np.mean(raw_stds) else 'â†‘å¢å¤§'}")
        print(f"    â”œâ”€â”€ æœ€å°å€¼: [{np.mean(raw_mins)} â€”â€”> {np.mean(processed_mins)}] "
              f"{'â†“å‡å°' if np.mean(processed_mins) < np.mean(raw_mins) else 'â†‘å¢å¤§'}")
        print(f"    â”œâ”€â”€ æœ€å¤§å€¼: [{np.mean(raw_maxs)} â€”â€”> {np.mean(processed_maxs)}] "
              f"{'â†“å‡å°' if np.mean(processed_maxs) < np.mean(raw_maxs) else 'â†‘å¢å¤§'}")

    @staticmethod
    def _print_mask_statical(raw_mask_stats_list: List[Dict], processed_mask_stats_list: List[Dict]):
        """æ‰“å°æ©ç åˆ†å¸ƒæ‘˜è¦"""
        # æ”¶é›†å‰æ™¯æ¯”ä¾‹
        raw_foreground_ratios = []
        processed_foreground_ratios = []

        for raw_stats, processed_stats in zip(raw_mask_stats_list, processed_mask_stats_list):
            # ç±»åˆ«0æ˜¯å‰æ™¯
            if 'åŸå§‹æ©ç _class_0_percentage' in raw_stats:
                raw_foreground_ratios.append(raw_stats['åŸå§‹æ©ç _class_0_percentage'])
            if 'å½’ä¸€åŒ–åæ©ç _class_0_percentage' in processed_stats:
                processed_foreground_ratios.append(processed_stats['å½’ä¸€åŒ–åæ©ç _class_0_percentage'])

        if raw_foreground_ratios:
            print(f"  å‰æ™¯åƒç´ æ¯”ä¾‹:")
            print(f"    â”œâ”€â”€ åŸå§‹æ•°æ®: {np.min(raw_foreground_ratios):5.2f}%~{np.max(raw_foreground_ratios):5.2f}% (å¹³å‡: {np.mean(raw_foreground_ratios):5.2f}%)")
            print(f"    â””â”€â”€ å¤„ç†åæ•°æ®: {np.min(processed_foreground_ratios):5.2f}%~{np.max(processed_foreground_ratios):5.2f}% (å¹³å‡: {np.mean(processed_foreground_ratios):5.2f}%)")


    @staticmethod
    def _print_individual_sample_stats(dataset_stats: Dict[str, Any]):
        """æ‰“å°å•ä¸ªæ ·æœ¬çš„è¯¦ç»†ç»Ÿè®¡"""
        file_pairs = dataset_stats.get('file_pairs', [])
        if not file_pairs:
            return

        print(f"\nğŸ” å•ä¸ªæ ·æœ¬è¯¦ç»†ç»Ÿè®¡:")
        print("=" * 80)

        sample_count = 0
        for file_idx, file_info in enumerate(file_pairs):
            image_file = os.path.basename(file_info['image_file'])

            # åªæ˜¾ç¤ºå‰å‡ ä¸ªæ ·æœ¬çš„è¯¦ç»†ä¿¡æ¯ï¼ˆé¿å…è¾“å‡ºè¿‡é•¿ï¼‰
            if sample_count >= 10:  # æœ€å¤šæ˜¾ç¤º10ä¸ªæ ·æœ¬çš„è¯¦ç»†ä¿¡æ¯
                print(f"... è¿˜æœ‰ {len(file_pairs) - 10} ä¸ªæ–‡ä»¶çš„ç»Ÿè®¡ä¿¡æ¯æœªæ˜¾ç¤º")
                break

            print(f"\nğŸ“ æ–‡ä»¶ {file_idx + 1}: {image_file}")
            print("-" * 50)

            # åŸå§‹æ•°æ®ç»Ÿè®¡
            print("ğŸ”¹ åŸå§‹æ•°æ®:")
            raw_image_stats = file_info['image_raw_stats']
            print(f"   å½¢çŠ¶: {file_info['image_shape']['åŸå§‹å›¾åƒ_shape']}")
            print(f"   èŒƒå›´: [{raw_image_stats['åŸå§‹å›¾åƒ_min']:6.4f}, {raw_image_stats['åŸå§‹å›¾åƒ_max']:6.4f}]")
            print(f"   å‡å€¼: {raw_image_stats['åŸå§‹å›¾åƒ_mean']:6.4f}")
            print(f"   æ ‡å‡†å·®: {raw_image_stats['åŸå§‹å›¾åƒ_std']:6.4f}")

            # å¤„ç†åæ•°æ®ç»Ÿè®¡
            print("ğŸ”¹ å¤„ç†åæ•°æ®:")
            processed_image_stats = file_info['image_processed_stats']
            print(f"   èŒƒå›´: [{processed_image_stats['å½’ä¸€åŒ–åå›¾åƒ_min']:6.4f}, {processed_image_stats['å½’ä¸€åŒ–åå›¾åƒ_max']:6.4f}]")
            print(f"   å‡å€¼: {processed_image_stats['å½’ä¸€åŒ–åå›¾åƒ_mean']:6.4f}")
            print(f"   æ ‡å‡†å·®: {processed_image_stats['å½’ä¸€åŒ–åå›¾åƒ_std']:6.4f}")

            # æ©ç ç»Ÿè®¡
            print("ğŸ”¹ æ©ç åˆ†å¸ƒ:")
            raw_mask_stats = file_info['mask_raw_stats']
            processed_mask_stats = file_info['mask_processed_stats']

            # æ˜¾ç¤ºå„ç±»åˆ«æ¯”ä¾‹
            for k, v in raw_mask_stats.items():
                if 'percentage' in k:
                    class_name = k.split('_')[2]  # æå–ç±»åˆ«å
                    print(f"   ç±»åˆ«{class_name}: {v:5.2f}% (åŸå§‹)")

            for k, v in processed_mask_stats.items():
                if 'percentage' in k:
                    class_name = k.split('_')[2]
                    print(f"   ç±»åˆ«{class_name}: {v:5.2f}% (å¤„ç†å)")

            sample_count += 1

    @staticmethod
    def print_anomaly_check(dataset_stats: Dict[str, Any]):
        """æ‰“å°å¼‚å¸¸æ£€æŸ¥ç»“æœ"""
        file_pairs = dataset_stats.get('file_pairs', [])
        if not file_pairs:
            return

        print(f"\nâš ï¸  å¼‚å¸¸æ£€æŸ¥:")
        print("-" * 60)

        has_nan_files = []
        has_inf_files = []

        for i, file_info in enumerate(file_pairs):
            raw_stats = file_info['image_raw_stats']
            processed_stats = file_info['image_processed_stats']

            if raw_stats.get('åŸå§‹å›¾åƒ_has_nan', False) or processed_stats.get('å½’ä¸€åŒ–åå›¾åƒ_has_nan', False):
                has_nan_files.append(os.path.basename(file_info['image_file']))

            if raw_stats.get('åŸå§‹å›¾åƒ_has_inf', False) or processed_stats.get('å½’ä¸€åŒ–åå›¾åƒ_has_inf', False):
                has_inf_files.append(os.path.basename(file_info['image_file']))

        if has_nan_files:
            print(f"âŒ å‘ç° NaN å€¼çš„æ–‡ä»¶: {has_nan_files}")
        else:
            print("âœ… æœªå‘ç° NaN å€¼")

        if has_inf_files:
            print(f"âŒ å‘ç° Inf å€¼çš„æ–‡ä»¶: {has_inf_files}")
        else:
            print("âœ… æœªå‘ç° Inf å€¼")



class NIIDataProcessor:
    """åŒ»å­¦å›¾åƒæ•°æ®å¤„ç†å™¨ - æ”¯æŒåŠ¨æ€åˆ‡ç‰‡æ•°"""

    @staticmethod
    def trans_datatype_float32(data):
        return data.astype(np.float32)

    @staticmethod
    def concatenated_multislice(volume, center_slice, num_slices=3):
        """
        æå–å¤šä¸ªç›¸é‚»åˆ‡ç‰‡ï¼Œç”¨äºè·¨åˆ‡ç‰‡ä¸Šä¸‹æ–‡

        å‚æ•°:
            volume: 3Dä½“ç§¯æ•°æ® [H, W, D]
            center_slice: ä¸­å¿ƒåˆ‡ç‰‡ç´¢å¼•
            num_slices: æå–çš„åˆ‡ç‰‡æ•°é‡ï¼ˆå¿…é¡»ä¸ºå¥‡æ•°ï¼‰

        è¿”å›:
            å¤šåˆ‡ç‰‡æ•°æ® [H, W, num_slices]
        """
        if num_slices % 2 == 0:
            raise ValueError("num_slices å¿…é¡»ä¸ºå¥‡æ•°ä»¥ä¿è¯å¯¹ç§°")

        half = num_slices // 2
        total_slices = volume.shape[2]
        slices = []

        # è®¡ç®—éœ€è¦æå–çš„åˆ‡ç‰‡ç´¢å¼•èŒƒå›´
        target_indices = []
        for offset in range(-half, half + 1):
            target_idx = center_slice + offset
            # è¾¹ç•Œå¤„ç†ï¼šè¶…å‡ºèŒƒå›´çš„ç´¢å¼•æ˜ å°„åˆ°æœ€è¿‘çš„è¾¹ç•Œ
            if target_idx < 0:
                target_idx = 0
            elif target_idx >= total_slices:
                target_idx = total_slices - 1
            target_indices.append(target_idx)

        # æå–åˆ‡ç‰‡
        for idx in target_indices:
            slices.append(volume[:, :, idx])

        return np.stack(slices, axis=-1)  # [H, W, num_slices]

    @staticmethod
    def nii_normalize(images, masks):

        # å¯¹æ¯ä¸ªåˆ‡ç‰‡çš„é€šé“è¿›è¡Œå½’ä¸€åŒ–
        means = images.mean(axis=(0, 1))  # æ¯ä¸ªåˆ‡ç‰‡çš„å‡å€¼
        stds = images.std(axis=(0, 1))    # æ¯ä¸ªåˆ‡ç‰‡çš„æ ‡å‡†å·®

        processed_images = (images - means) / (stds + 1e-8)
        processed_masks = (masks > 0).astype(np.float32)

        return processed_images, processed_masks


class CatNiiDataset(SegDataset):
    """
    CatNiiDataset - ä¿æŒè·¨åˆ‡ç‰‡å¤„ç†å¹¶å…¼å®¹transforms
    """

    def __init__(self, img_dir: str, mask_dir: str, transforms: List = [],
                 check: str = 'none', num_slices: int = 3, enable_inspection: bool = False):
        """
        å‚æ•°:
            img_dir: å›¾åƒç›®å½•
            mask_dir: æ©ç ç›®å½•
            transforms: æ•°æ®è½¬æ¢åˆ—è¡¨
            check: æ£€æŸ¥æ¨¡å¼
            num_slices: è·¨åˆ‡ç‰‡æ•°é‡
            enable_inspection: æ˜¯å¦å¯ç”¨æ•°æ®æ£€æŸ¥
        """
        self.num_slices = num_slices
        self.enable_inspection = enable_inspection
        self.dataset_stats = {}  # æ•°æ®é›†çº§åˆ«ç»Ÿè®¡ä¿¡æ¯

        # è°ƒç”¨çˆ¶ç±»åˆå§‹åŒ–
        super().__init__(img_dir, mask_dir, transforms, check)

        # åˆå§‹åŒ–å®Œæˆåè¿›è¡Œæ•°æ®æ£€æŸ¥
        # if enable_inspection:
        #     self._perform_dataset_analysis()

    def _resolve_ids(self, img_dir, mask_dir, check='none'):
        """æ–‡ä»¶è§£æï¼ŒåŠ è½½NIfTIæ•°æ®"""
        image_mask_pairs = super()._resolve_ids(img_dir, mask_dir, check)
        self.load_datas = {}

        # è®°å½•æ•°æ®é›†åŸºæœ¬ä¿¡æ¯
        if self.enable_inspection:
            self.dataset_stats['total_files'] = len(image_mask_pairs)
            self.dataset_stats['file_pairs'] = []

        # é¢„åŠ è½½æ‰€æœ‰NIfTIæ–‡ä»¶ï¼Œä¿æŒåŸå§‹float32ç²¾åº¦
        for image_file, mask_file in image_mask_pairs:
            # åŠ è½½å›¾åƒæ•°æ®
            image_raw_data = nib.load(image_file).get_fdata()
            mask_raw_data = nib.load(mask_file).get_fdata()

            # è®°å½•æ–‡ä»¶çº§åˆ«ä¿¡æ¯ - åŸå§‹åŠ è½½æ•°æ®
            file_info = {
                'image_file': image_file,
                'mask_file': mask_file
            }

            # åªæœ‰åœ¨å¯ç”¨æ£€æŸ¥æ—¶æ‰å¡«å……ç»Ÿè®¡ä¿¡æ¯
            if self.enable_inspection:
                # åŸå§‹æ•°æ®ç»Ÿè®¡
                file_info.update({
                    'image_shape': DataInspector.get_shape_info(image_raw_data, "åŸå§‹å›¾åƒ"),
                    'mask_shape': DataInspector.get_shape_info(mask_raw_data, "åŸå§‹æ©ç "),
                    'image_raw_stats': DataInspector.get_image_statistical(image_raw_data, "åŸå§‹å›¾åƒ"),
                    'mask_raw_stats': DataInspector.get_mask_distribution(mask_raw_data, "åŸå§‹æ©ç ")
                })

            # åŸºç¡€ç±»å‹è½¬æ¢å¤„ç†
            image_processed_data = NIIDataProcessor.trans_datatype_float32(image_raw_data)
            mask_processed_data = NIIDataProcessor.trans_datatype_float32(mask_raw_data)

            # niiæ•°æ®çš„å½’ä¸€åŒ–å¤„ç†
            image_processed_data, mask_processed_data = NIIDataProcessor.nii_normalize(image_processed_data, mask_processed_data)

            # æ”¶å½•æ•°æ®
            self.load_datas[image_file] = image_processed_data
            self.load_datas[mask_file] = mask_processed_data

            # æ·»åŠ å¤„ç†åä¿¡æ¯
            if self.enable_inspection:
                processed_info = {
                    'image_processed_stats': DataInspector.get_image_statistical(image_processed_data, "å½’ä¸€åŒ–åå›¾åƒ"),
                    'mask_processed_stats': DataInspector.get_mask_distribution(mask_processed_data, "å½’ä¸€åŒ–åæ©ç ")
                }
                file_info.update(processed_info)  # å°†å¤„ç†åçš„ä¿¡æ¯åˆå¹¶åˆ°åŸæœ‰å­—å…¸      ### ï¼Ÿæç¤ºâ€œå±€éƒ¨å˜é‡ 'file_info' å¯èƒ½åœ¨èµ‹å€¼å‰å¼•ç”¨ â€ï¼Œè¿™ä¸ªè®¾è®¡å…¶å®æ„Ÿè§‰ä¸å¤ªèˆ’æœï¼Œä½†å¦‚æœä¸åœ¨å‰åéƒ½åŠ â€œself.enable_inspectionâ€åˆä¸å¯¹ï¼Œå¾…ç©¶
                self.dataset_stats['file_pairs'].append(file_info)


        # åˆ›å»ºæ ·æœ¬ç´¢å¼•å¯¹ (æ–‡ä»¶, æ–‡ä»¶, åˆ‡ç‰‡å±‚)
        id_pairs = []

        for image_file, mask_file in image_mask_pairs:
            if self.load_datas[image_file].shape[2] != self.load_datas[mask_file].shape[2]:
                print(f"âš ï¸ è­¦å‘Š: {image_file} å’Œ {mask_file} åˆ‡ç‰‡æ•°é‡ä¸åŒ¹é…")
                num_layers = min(self.load_datas[image_file].shape[2], self.load_datas[mask_file].shape[2])
            else:
                num_layers = self.load_datas[image_file].shape[2]
            for layer in range(num_layers):
                id_pairs.append((image_file, mask_file, layer))
        self.dataset_stats['total_samples'] = len(id_pairs)
        return id_pairs

    def _load_datas(self, id):
        """é‡å†™æ•°æ®åŠ è½½ï¼Œä¿æŒè·¨åˆ‡ç‰‡å¤„ç†"""
        image_file, mask_file, layer = id
        image_data = self.load_datas[image_file]
        mask_data = self.load_datas[mask_file]

        # æå–å¤šä¸ªç›¸é‚»åˆ‡ç‰‡ç”¨äºä¸Šä¸‹æ–‡ä¿¡æ¯
        processed_images = NIIDataProcessor.concatenated_multislice(
            image_data, layer, self.num_slices
        )
        # æ©ç åªä½¿ç”¨ä¸­å¿ƒåˆ‡ç‰‡ï¼Œä½†ä¿æŒç›¸åŒçš„å¤„ç†æ¥å£
        processed_masks = mask_data[:, :, [layer]]  # [H, W, 1]

        return processed_images, processed_masks

    def get_dataset_stats(self):
        """è·å–æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯"""
        stats = self.dataset_stats.copy()
        if stats:
            return stats
        else:
            return None