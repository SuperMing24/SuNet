"""è®­ç»ƒå™¨ - åè°ƒæ‰€æœ‰ç»„ä»¶"""
import os
import time
import math
import json
import csv
from typing import Dict, Any, Optional, Union
from datetime import datetime

import torch
import torch.optim as optim
from IPython.conftest import work_path
from torch.optim.lr_scheduler import ReduceLROnPlateau

class ModelManager:
    """æ¨¡å‹ç®¡ç†å™¨ - ä¸“æ³¨æ¨¡å‹ä¿å­˜åŠ è½½"""

    def __init__(self, save_dir: str = "checkpoints"):
        self.save_dir = save_dir
        os.makedirs(save_dir, exist_ok=True)

    def save(self, model: torch.nn.Module, optimizer: optim.Optimizer,
             epoch: int, filename: str, **kwargs):
        """ä¿å­˜æ¨¡å‹æ£€æŸ¥ç‚¹"""
        state = {
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'epoch': epoch,
            **kwargs
        }
        torch.save(state, os.path.join(self.save_dir, filename))

    def load(self, model: torch.nn.Module, optimizer: optim.Optimizer,
             filepath: str) -> Optional[Dict[str, Any]]:
        """åŠ è½½æ¨¡å‹æ£€æŸ¥ç‚¹ - å¥å£®å®ç°"""
        filepath = os.path.join(self.save_dir, filepath)
        if not os.path.exists(filepath):
            print(f"æ£€æŸ¥ç‚¹ä¸å­˜åœ¨: '{filepath}'")
            return None

        try:
            # è‡ªåŠ¨è®¾å¤‡æ˜ å°„
            map_location = 'cuda' if torch.cuda.is_available() else 'cpu'
            state = torch.load(filepath, map_location=map_location)

            model.load_state_dict(state['model_state_dict'])
            optimizer.load_state_dict(state['optimizer_state_dict'])

            print(f"æˆåŠŸåŠ è½½æ£€æŸ¥ç‚¹ (epoch {state.get('epoch', 'unknown')})")
            return {'epoch': state.get('epoch', 0), **state}
        except Exception as e:
            print(f"åŠ è½½æ£€æŸ¥ç‚¹å¤±è´¥: {e}")
            return None

class TrainingResultManager:
    """è®­ç»ƒç»“æœç®¡ç†å™¨ - ä¸“æ³¨ç»“æ„åŒ–æ•°æ®ä¿å­˜"""

    def __init__(self, save_dir: str = "results"):
        self.save_dir = save_dir
        os.makedirs(save_dir, exist_ok=True)

        # CSVæ–‡ä»¶è·¯å¾„
        self.csv_file = os.path.join(save_dir, "training_results.csv")
        self._init_csv_file()

        # JSONå¤‡ä»½æ–‡ä»¶è·¯å¾„
        self.json_file = os.path.join(save_dir, "training_results.json")

    def _init_csv_file(self):
        """åˆå§‹åŒ–CSVæ–‡ä»¶è¡¨å¤´"""
        if not os.path.exists(self.csv_file):
            with open(self.csv_file, 'w', newline='', encoding='utf-8') as f:
                writer = csv.writer(f)
                # å®šä¹‰è¡¨å¤´
                headers = [
                    'epoch', 'timestamp', 'train_loss', 'val_loss',
                    'learning_rate', 'epoch_duration', 'gradient_norm',
                    # è¯„ä¼°æŒ‡æ ‡
                    'dice', 'cl_dice', 'hausdorff', 'accuracy',
                    # æ¨¡å‹æŒ‡æ ‡
                    'best_metric'
                ]
                writer.writerow(headers)

    def save_epoch_result(self, epoch_data: Dict[str, Any]):
        """ä¿å­˜epochè®­ç»ƒç»“æœåˆ°CSVå’ŒJSON"""
        # ä¿å­˜åˆ°CSV
        self._save_to_csv(epoch_data)

        # ä¿å­˜åˆ°JSONï¼ˆè¿½åŠ æ¨¡å¼ï¼‰
        # self._save_to_json(epoch_data)

    def _save_to_csv(self, data: Dict[str, Any]):
        """ä¿å­˜åˆ°CSVæ–‡ä»¶"""
        with open(self.csv_file, 'a', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)

            # æŒ‰è¡¨å¤´é¡ºåºæå–æ•°æ®
            row = [
                data.get('epoch', ''),
                data.get('timestamp', ''),
                data.get('train_loss', ''),
                data.get('val_loss', ''),
                data.get('learning_rate', ''),
                data.get('epoch_duration', ''),
                data.get('gradient_norm', ''),
                # è¯„ä¼°æŒ‡æ ‡
                data.get('eval_metrics', {}).get('dice', ''),
                data.get('eval_metrics', {}).get('cl_dice', ''),
                data.get('eval_metrics', {}).get('hausdorff', ''),
                data.get('eval_metrics', {}).get('accuracy', ''),
                # æ¨¡å‹çŠ¶æ€
                data.get('best_metric', '')
            ]
            writer.writerow(row)

    def _save_to_json(self, data: Dict[str, Any]):
        """ä¿å­˜åˆ°JSONæ–‡ä»¶ï¼ˆè¿½åŠ æ¨¡å¼ï¼‰"""
        # è¯»å–ç°æœ‰æ•°æ®
        all_data = []
        if os.path.exists(self.json_file):
            try:
                with open(self.json_file, 'r', encoding='utf-8') as f:
                    all_data = json.load(f)
            except (json.JSONDecodeError, Exception):
                all_data = []

        # æ·»åŠ æ–°æ•°æ®
        all_data.append(data)

        # å†™å›æ–‡ä»¶
        with open(self.json_file, 'w', encoding='utf-8') as f:
            json.dump(all_data, f, indent=2, ensure_ascii=False)

class Trainer:
    prompt = '(trainer) '

    def __init__(self, config: Dict[str, Any]):
        super().__init__()
        self.config = config
        self.device = config.get('device', torch.device('cuda' if torch.cuda.is_available() else 'cpu'))

        # åˆå§‹åŒ–æ¨¡å‹
        self.model = config['model'].to(self.device)
        self.optimizer = optim.Adam(self.model.parameters(), lr=config.get('lr', 1e-4))
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        self.scheduler = ReduceLROnPlateau(self.optimizer, 'min', patience=5)

        # æŸå¤±å‡½æ•° - è¡€ç®¡åˆ†å‰²é»˜è®¤ä½¿ç”¨clDice
        self.loss_fn = config.get('loss_fn')
        self.loss_id = config.get('loss_id')

        # è¯„ä¼°ç³»ç»Ÿ
        self.metric_evaluator = config.get('metric_evaluator')
        self.best_metric = float('inf')

        # æ—¥å¿—ç³»ç»Ÿ
        self.loggers = config.get('loggers')
        self.model_manager = ModelManager(config.get('model_dir'))

        #å¯è§†åŒ–å·¥å…·
        self.visualizer = config.get('visualizer')
        self.interval_visualize = config.get('interval_visualize')
        self.image_dir = os.path.join(config.get('work_dir'), 'image')

        # è®­ç»ƒç»“æœç®¡ç†å™¨
        self.result_manager = TrainingResultManager(config.get('model_dir'))

        # è®­ç»ƒçŠ¶æ€
        self.current_epoch = 1
        self.train_loader = None
        self.val_loader = None
        self.train_losses = []  # è®°å½•æ¯ä¸ªepochçš„è®­ç»ƒæŸå¤±

        self.training_actions = config.get('training_actions')

        # æ—©åœæœºåˆ¶
        self.patience = config.get('patience', 10)
        self.early_stop_counter = 0

        # å‚æ•°å­˜æ¡£ç‚¹æ£€æŸ¥ï¼Œä¸æ¢å¤
        if 'checkpoint' in config:
            self._load_checkpoint(config['checkpoint'])

        # è¯„ä¼°è´¨é‡æ£€æŸ¥
        self.enable_quality_checks = config.get('enable_quality_checks', False)
        # è®­ç»ƒçŠ¶æ€æ£€æŸ¥
        self.training = False

    def _log(self, method_name: str, *args, **kwargs):
        """ç»Ÿä¸€çš„æ—¥å¿—è®°å½•æ–¹æ³•"""
        for logger in self.loggers:
            if hasattr(logger, method_name):
                log_method = getattr(logger, method_name)
                try:
                    # åŒæ—¶ä¼ é€’ä½ç½®å‚æ•°å’Œå…³é”®å­—å‚æ•°
                    log_method(*args, **kwargs)
                except TypeError:
                    # å‚æ•°ä¸åŒ¹é…ï¼Œå°è¯•ç®€åŒ–è°ƒç”¨
                    try:
                        log_method(*args)  # åªä¼ ä½ç½®å‚æ•°
                    except Exception as e:
                        print(f"æ—¥å¿—è®°å½•é”™è¯¯ ({method_name}): {e}")
                except Exception as e:
                    print(f"æ—¥å¿—è®°å½•é”™è¯¯ ({method_name}): {e}")

    def _load_checkpoint(self, checkpoint_path: str):
        """åŠ è½½æ£€æŸ¥ç‚¹"""
        state = self.model_manager.load(self.model, self.optimizer, checkpoint_path)
        if state:
            self.current_epoch = state['epoch'] + 1  # ä»ä¸‹ä¸€è½®å¼€å§‹
            self.best_metric = state.get('best_metric', self.best_metric)
            print(f"ä»epoch {state['epoch']}æ¢å¤è®­ç»ƒ")

    def _train_epoch(self) -> float:
        """è®­ç»ƒé˜¶æ®µ - ä½¿ç”¨kwargså‚æ•°"""
        self.model.train()
        total_loss = 0.0

        for batch_idx, (data, target) in enumerate(self.train_loader):
            start_time = time.time()
            data, target = data.to(self.device), target.to(self.device)

            self.optimizer.zero_grad()
            output = self.model(data)
            loss = self.loss_fn(output, target)
            loss.backward()

            # è®¡ç®—æ¢¯åº¦èŒƒæ•°ï¼ˆç”¨äºç›‘æ§ï¼‰
            total_norm = self._compute_gradient_norm()

            self.optimizer.step()

            total_loss += loss.item()
            use_time = time.time() - start_time

            # å¢åŠ kwargsä¼ é€’ä¿¡æ¯
            self._log('log_loss', 'train',
                    self.current_epoch, batch_idx, len(self.train_loader),loss.item(),
                    è€—æ—¶=f"{use_time:.2f}s",
                    æ¢¯åº¦èŒƒæ•°=f"{total_norm:.4f}",
                    è®¾å¤‡=str(self.device),
                    ä¸»è¦è¯„ä¼°=f"{self.best_metric}")

            # å¯è§†åŒ–
            if batch_idx % self.interval_visualize == 0:
                self.visualizer.plot(data[0], target[0], output[0],
                                     f'{self.image_dir}/epoch_{self.current_epoch}_batch_{batch_idx}.png', (8, 8))

            if not self.training:
                break

        # è¿”å›æ•´ä¸ªepochçš„å¹³å‡è®­ç»ƒæŸå¤±
        avg_train_loss = total_loss / len(self.train_loader)
        return avg_train_loss

    def _validate(self):
        """éªŒè¯é˜¶æ®µ - è®¡ç®—æ‰€æœ‰è¯„ä¼°æŒ‡æ ‡"""
        self.model.eval()
        all_outputs = []
        all_targets = []
        total_val_loss = 0.0

        with torch.no_grad():
            for batch_idx, (data, target) in enumerate(self.val_loader):
                start_time = time.time()
                # åŒæ—¶ç§»åŠ¨dataå’Œtargetåˆ°è®¾å¤‡
                data, target = data.to(self.device), target.to(self.device)
                output = self.model(data)
                all_outputs.append(output.cpu())
                all_targets.append(target.cpu())

                # è®°å½•éªŒè¯æŸå¤±å’Œè€—æ—¶ç­‰ä¿¡æ¯
                val_loss = self.loss_fn(output, target).item()
                total_val_loss += val_loss
                use_time = time.time() - start_time
                self._log('log_loss', 'val',
                        self.current_epoch, batch_idx, len(self.val_loader), val_loss,
                        è€—æ—¶=f"{use_time:.2f}s",
                        æ‰¹æ¬¡å¤§å°=f"{data.size(0)}")

        # è®¡ç®—å¹³å‡éªŒè¯æŸå¤±
        avg_val_loss = total_val_loss / len(self.val_loader)

        # åˆå¹¶è®¡ç®—è¯„ä¼°æŒ‡æ ‡ï¼Œä¸åŒ…å«æŸå¤±å€¼
        eval_metrics = {}
        if self.metric_evaluator:
            combined_output = torch.cat(all_outputs, dim=0)
            combined_target = torch.cat(all_targets, dim=0)
            eval_metrics = self.metric_evaluator.evaluate(combined_output, combined_target)

            # è®°å½•è¯„ä¼°æŒ‡æ ‡
            self._log('log_metrics', self.current_epoch, eval_metrics,
                    å­¦ä¹ ç‡=f"{self.optimizer.param_groups[0]['lr']:.2e}",
                    éªŒè¯æ ·æœ¬æ€»æ•°=f"{combined_output.size(0)}",
                    æœ€ä½³æŒ‡æ ‡=f"{self.best_metric:.4f}",)

        # è¿”å›éªŒè¯æŸå¤±å’Œæ‰€æœ‰æŒ‡æ ‡
        return avg_val_loss, eval_metrics

    def _compute_gradient_norm(self) -> float:
        """è®¡ç®—æ¢¯åº¦L2èŒƒæ•°"""
        total_norm = 0.0
        for p in self.model.parameters():
            if p.grad is not None:
                param_norm = p.grad.data.norm(2)  # L2èŒƒæ•°
                total_norm += param_norm.item() ** 2
        return total_norm ** 0.5

    def _should_save_model(self, loss_metrics: Dict[str, float], eval_metrics: Dict[str, float]) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥ä¿å­˜æ¨¡å‹"""

        # 1. é¦–å…ˆæ£€æŸ¥æ˜¯å¦æœ‰æ”¹è¿›
        if not self._is_improvement(loss_metrics):
            return False

        # 2. å¦‚æœå¯ç”¨äº†è´¨é‡æ£€æŸ¥ï¼Œè¿”å›è´¨é‡æ£€æŸ¥ç»“æœï¼›å¦åˆ™ç›´æ¥è¿”å›True
        return self._check_quality_issue(eval_metrics) if self.enable_quality_checks else True

    def _is_improvement(self, loss_metrics: Dict[str, float]) -> bool:
        """æœ‰æ”¹è¿›çš„æ ‡å‡†ï¼šéªŒè¯é˜¶æ®µæŸå¤±å€¼æ›´å°"""
        val_loss = loss_metrics['val_loss']
        return val_loss < self.best_metric

    def _check_quality_issue(self, eval_metrics: Dict[str, float]) -> bool:
        """è´¨é‡æ£€æŸ¥ - ä¸¥æ ¼çš„å¤šæŒ‡æ ‡æ£€æŸ¥"""

        # æ£€æŸ¥1: åŸºç¡€æ•°å€¼æœ‰æ•ˆæ€§
        if not self._check_numerical_validity(eval_metrics):
            return False

        # æ£€æŸ¥2: å…³é”®æŒ‡æ ‡åˆç†æ€§
        if not self._check_key_metrics_reasonable(eval_metrics):
            return False

        # æ£€æŸ¥3: æŒ‡æ ‡é—´ä¸€è‡´æ€§
        if not self._check_metrics_consistency(eval_metrics):
            return False

        # æ£€æŸ¥4: ä»»åŠ¡ç‰¹å®šæ£€æŸ¥
        if not self._check_task_specific_rules(eval_metrics):
            return False

        return True

    def _save_better_model(self, epoch: int, loss_metrics: Dict[str, float]):
        """ä¿å­˜æœ€ä½³æ¨¡å‹ - åªå…³æ³¨æ¨¡å‹ä¼˜åŒ–"""
        self.best_metric = loss_metrics['val_loss']

        # ä¿å­˜æ¨¡å‹
        self.model_manager.save(
            self.model, self.optimizer, epoch,
            'best_model.pth',
            best_metric=self.best_metric
        )

        # è®°å½•ä¿å­˜ä¿¡æ¯
        metric_info = f"{self.loss_id} | val_loss: {self.best_metric:.4f}"
        # è€ƒè™‘æ˜¯å¦è¦å†åŠ train_lossï¼Ÿï¼Ÿ
        self._log('log_time', f"ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹ ({metric_info})")

    def _check_early_stop(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦åº”è¯¥æ—©åœ"""
        self.early_stop_counter += 1
        if self.early_stop_counter >= self.patience:
            self._log('log_time', f"ğŸ›‘ æ—©åœè§¦å‘ï¼Œè¿ç»­{self.patience}ä¸ªepochæœªæ”¹å–„")
            return True
        return False

    def _update_learning_rate(self, val_loss: float):
        """æ›´æ–°å­¦ä¹ ç‡ - ä½¿ç”¨éªŒè¯æŸå¤±"""
        if not math.isnan(val_loss) and val_loss != float('inf'):
            self.scheduler.step(val_loss)

            # è®°å½•å­¦ä¹ ç‡å˜åŒ–ï¼ˆå¯é€‰ï¼‰
            current_lr = self.optimizer.param_groups[0]['lr']
            self._log('log_time', f"ğŸ“‰ å­¦ä¹ ç‡ç°ä¸º: {current_lr:.2e}")

    def _save_epoch_results(self, loss_metrics: Dict[str, float], eval_metrics: Dict[str, float],
                        epoch_duration: float):
        """ä¿å­˜epochè®­ç»ƒç»“æœ - ç»“æ„åŒ–æ•°æ®"""
        epoch_data = {
            'epoch': self.current_epoch,
            'timestamp': datetime.now().isoformat(),
            'train_loss': loss_metrics['train_loss'],
            'val_loss': loss_metrics['val_loss'],
            'learning_rate': self.optimizer.param_groups[0]['lr'],
            'epoch_duration': epoch_duration,
            'gradient_norm': self._compute_gradient_norm(),
            'eval_metrics': eval_metrics,
            'best_metric': self.best_metric,
        }

        # ä½¿ç”¨ç»“æœç®¡ç†å™¨ä¿å­˜
        self.result_manager.save_epoch_result(epoch_data)

        # åŒæ—¶è®°å½•åˆ°æ—¥å¿—
        self._log('log_time',
                 f"ğŸ“Š Epoch {self.current_epoch} ç»“æœå·²ä¿å­˜ | "
                 f"Train: {loss_metrics['train_loss']:.4f} | Val: {loss_metrics['val_loss']:.4f} | ")

    def _evaluate_training_progress(self, epoch: int,
            loss_metrics: Dict[str, float], eval_metrics: Dict[str, float]) -> Dict[str, Any]:
        """è¯„ä¼°è®­ç»ƒè¿›åº¦ - åªæ”¶é›†çŠ¶æ€ä¿¡æ¯ï¼Œä¸æ‰§è¡Œæ“ä½œ"""

        progress_info = {
            'epoch': epoch,
            'should_update_lr': self.training_actions.get('update_lr_every_epoch', True),
            'should_save_model': self._should_save_model(loss_metrics, eval_metrics),
            'should_early_stop': False,
        }

        # æ£€æŸ¥æ—©åœ
        if not progress_info['should_save_model'] and self.training_actions.get('enable_early_stop'):
            progress_info['should_early_stop'] = self._check_early_stop()

        return progress_info

    def _execute_training_actions(self, progress_info: Dict[str, Any],
                loss_metrics: Dict[str, float], eval_metrics: Dict[str, float], epoch_duration: float):
        """é…ç½®é©±åŠ¨çš„è®­ç»ƒåŠ¨ä½œæ‰§è¡Œ"""

        # 1. æ›´æ–°å­¦ä¹ ç‡
        if progress_info['should_update_lr']:
            self._update_learning_rate(loss_metrics['val_loss'])

        # 2. ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ˆæ¨¡å‹ä¼˜åŒ–ç­–ç•¥ï¼‰
        if progress_info['should_save_model']:
            self._save_better_model(
                progress_info['epoch'],
                loss_metrics
            )
            self.early_stop_counter = 0

        # 3. ä¿å­˜å®šæœŸç»“æœæ¡£æ¡ˆï¼ˆç»“æœåˆ†æç­–ç•¥ï¼‰
        if self.training_actions.get('should_save_checkpoint', True):
            # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
            if progress_info['epoch'] % self.training_actions.get('save_checkpoint_interval') == 0:
                # ä¿å­˜è®­ç»ƒç»“æœæ•°æ®
                self._save_epoch_results(loss_metrics, eval_metrics, epoch_duration)

        # 4. å¤„ç†æ—©åœ
        if progress_info.get('should_early_stop', False):
            self._log('log_time', f"ğŸ›‘ æ—©åœè§¦å‘ï¼Œè¿ç»­{self.patience}ä¸ªepochæœªæ”¹å–„")

    def start_train(self):
        """å¼€å§‹è®­ç»ƒ - ä½¿ç”¨éªŒè¯æŒ‡æ ‡é€‰æ‹©æœ€ä½³æ¨¡å‹"""
        self.train_loader = self.config['train_loader']
        self.val_loader = self.config['val_loader']
        self.training = True

        # è®°å½•è®­ç»ƒå¼€å§‹æ—¶é—´
        total_start_time = time.time()
        self._log('log_time', "ğŸš€ è®­ç»ƒå¼€å§‹!", æ€»epochæ•°=f"{self.config['epochs']}")

        for epoch in range(self.current_epoch, self.config['epochs'] + 1):
            self.current_epoch = epoch
            loss_metrics = {}

            # è®°å½•epochå¼€å§‹æ—¶é—´
            epoch_start_time = time.time()

            # è®­ç»ƒé˜¶æ®µ
            loss_metrics['train_loss'] = self._train_epoch()

            # éªŒè¯é˜¶æ®µ
            val_loss, eval_metrics = self._validate()
            loss_metrics['val_loss'] = val_loss

            # è®°å½•epochæ€»è€—æ—¶
            epoch_duration = time.time() - epoch_start_time
            self._log('log_time', f"Epoch {epoch} æ€»è€—æ—¶: {epoch_duration:.2f}s")

            # è¯„ä¼°è®­ç»ƒè¿›åº¦
            progress_info = self._evaluate_training_progress(epoch, loss_metrics, eval_metrics)
            # å¹¶æ‰§è¡Œç›¸åº”æ“ä½œ
            self._execute_training_actions(progress_info, loss_metrics, eval_metrics, epoch_duration)

            if progress_info.get('should_early_stop', False):
                break

            if not self.training:
                break

        # è®°å½•è®­ç»ƒæ€»è€—æ—¶
        total_time = time.time() - total_start_time
        self._log('log_time', f"è®­ç»ƒå®Œæˆ! æ€»è€—æ—¶: {total_time:.2f}s")

        # ä¿å­˜æœ€ç»ˆæ¨¡å‹å’Œç»“æœ
        self._save_final_results(total_time)

    def _save_final_results(self, total_time: float):
        """ä¿å­˜æœ€ç»ˆè®­ç»ƒç»“æœ"""
        final_data = {
            'final_epoch': self.current_epoch,
            'total_training_time': total_time,
            'best_metric': self.best_metric,
            'final_train_loss': self.train_losses[-1] if self.train_losses else 0.0,
            'training_completed': True,
            'completion_time': datetime.now().isoformat()
        }

        # ä¿å­˜åˆ°JSON
        final_file = os.path.join(self.config['model_dir'], "final_training_summary.json")
        with open(final_file, 'w', encoding='utf-8') as f:
            json.dump(final_data, f, indent=2, ensure_ascii=False)

        self._log('log_time', f"ğŸ“„ æœ€ç»ˆè®­ç»ƒæ‘˜è¦å·²ä¿å­˜: {final_file}")

    def save(self):
        """ä¿å­˜æ£€æŸ¥ç‚¹ - ç»“æœåˆ†æç­–ç•¥"""
        filename = f'checkpoint_epoch_{self.current_epoch}.pth'
        self.model_manager.save(self.model, self.optimizer, self.current_epoch, filename)
        msg = f'save model to {filename}'
        print(msg)
        self._log('log_time', f"ğŸ’¾ æ£€æŸ¥ç‚¹å·²ä¿å­˜: {filename}")
        return msg

    def stop(self):
        """åœæ­¢è®­ç»ƒå¹¶ä¿å­˜å½“å‰çŠ¶æ€"""
        self.save()
        self.training = False
        self._log('log_time', "è®­ç»ƒåœæ­¢")
