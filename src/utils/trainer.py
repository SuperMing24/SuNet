"""è®­ç»ƒå™¨ - åè°ƒæ‰€æœ‰ç»„ä»¶"""
import os
import time
import math
from abc import abstractmethod
from typing import Dict, Any, Optional

import torch
import torch.optim as optim
from torch.optim.lr_scheduler import ReduceLROnPlateau

class ModelManager:
    """æ¨¡å‹ç®¡ç†å™¨ - ä¸“æ³¨æ¨¡å‹ä¿å­˜åŠ è½½"""

    def __init__(self, save_dir: str = "checkpoints"):
        self.save_dir = save_dir
        os.makedirs(save_dir, exist_ok=True)

    def save(self, model: torch.nn.Module, optimizer: optim.Optimizer,
             epoch: int, filename: str, **kwargs):
        """ä¿å­˜æ¨¡å‹æ£€æŸ¥ç‚¹"""
        state = {
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'epoch': epoch,
            **kwargs
        }
        torch.save(state, os.path.join(self.save_dir, filename))

    def load(self, model: torch.nn.Module, optimizer: optim.Optimizer,
             filepath: str) -> Optional[Dict[str, Any]]:
        """åŠ è½½æ¨¡å‹æ£€æŸ¥ç‚¹ - å¥å£®å®ç°"""
        filepath = os.path.join(self.save_dir, filepath)
        if not os.path.exists(filepath):
            print(f"æ£€æŸ¥ç‚¹ä¸å­˜åœ¨: '{filepath}'")
            return None

        try:
            # è‡ªåŠ¨è®¾å¤‡æ˜ å°„
            map_location = 'cuda' if torch.cuda.is_available() else 'cpu'
            state = torch.load(filepath, map_location=map_location)

            model.load_state_dict(state['model_state_dict'])
            optimizer.load_state_dict(state['optimizer_state_dict'])

            print(f"æˆåŠŸåŠ è½½æ£€æŸ¥ç‚¹ (epoch {state.get('epoch', 'unknown')})")
            return {'epoch': state.get('epoch', 0), **state}
        except Exception as e:
            print(f"åŠ è½½æ£€æŸ¥ç‚¹å¤±è´¥: {e}")
            return None

class Trainer:
    prompt = '(trainer) '

    def __init__(self, config: Dict[str, Any]):
        super().__init__()
        self.config = config
        self.device = config.get('device', torch.device('cuda' if torch.cuda.is_available() else 'cpu'))

        # åˆå§‹åŒ–æ¨¡å‹
        self.model = config['model'].to(self.device)
        self.optimizer = optim.Adam(self.model.parameters(), lr=config.get('lr', 1e-4))
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        self.scheduler = ReduceLROnPlateau(self.optimizer, 'min', patience=5)

        # æŸå¤±å‡½æ•° - è¡€ç®¡åˆ†å‰²é»˜è®¤ä½¿ç”¨clDice
        self.loss_fn = config.get('loss_fn')

        # è¯„ä¼°ç³»ç»Ÿ
        self.metric_evaluator = config.get('metric_evaluator')
        # ç›‘æ§æŒ‡æ ‡é…ç½®
        self.monitor_metric = config.get('monitor_metric', 'val_loss')
        self.monitor_mode = config.get('monitor_mode', 'min')  # 'min' or 'max'
        # æ ¹æ®ç›‘æ§æ¨¡å¼è®¾ç½®åˆå§‹æœ€ä½³å€¼
        if self.monitor_mode == 'min':
            self.best_metric = float('inf')
        else:  # 'max'
            self.best_metric = float('-inf')

        # æ—¥å¿—ç³»ç»Ÿ
        self.loggers = config.get('loggers')
        self.model_manager = ModelManager(config.get('model_dir'))

        # è®­ç»ƒçŠ¶æ€
        self.current_epoch = 1
        self.train_loader = None
        self.val_loader = None

        # æ—©åœæœºåˆ¶
        self.patience = config.get('patience', 10)
        self.early_stop_counter = 0

        # ä»æ£€æŸ¥ç‚¹æ¢å¤
        if 'checkpoint' in config:
            self._load_checkpoint(config['checkpoint'])

        self.training = False

    def _log(self, method_name: str, *args, **kwargs):
        """ç»Ÿä¸€çš„æ—¥å¿—è®°å½•æ–¹æ³•"""
        for logger in self.loggers:
            if hasattr(logger, method_name):
                log_method = getattr(logger, method_name)
                try:
                    # åŒæ—¶ä¼ é€’ä½ç½®å‚æ•°å’Œå…³é”®å­—å‚æ•°
                    log_method(*args, **kwargs)
                except TypeError as e:
                    # å¦‚æœå‚æ•°ä¸åŒ¹é…ï¼Œå°è¯•åªä¼ é€’ä½ç½®å‚æ•°
                    try:
                        log_method(*args)
                    except Exception as e2:
                        print(f"æ—¥å¿—è®°å½•é”™è¯¯ ({method_name}): {e2}")
                except Exception as e:
                    print(f"æ—¥å¿—è®°å½•é”™è¯¯ ({method_name}): {e}")

    def _load_checkpoint(self, checkpoint_path: str):
        """åŠ è½½æ£€æŸ¥ç‚¹"""
        state = self.model_manager.load(self.model, self.optimizer, checkpoint_path)
        if state:
            self.current_epoch = state['epoch'] + 1  # ä»ä¸‹ä¸€è½®å¼€å§‹
            self.best_metric = state.get('best_metric', self.best_metric)
            print(f"ä»epoch {state['epoch']}æ¢å¤è®­ç»ƒ")

    def _train_epoch(self) -> float:
        """è®­ç»ƒé˜¶æ®µ - ä½¿ç”¨çµæ´»çš„kwargså‚æ•°"""
        self.model.train()
        total_loss = 0.0

        for batch_idx, (data, target) in enumerate(self.train_loader):
            start_time = time.time()
            data, target = data.to(self.device), target.to(self.device)

            self.optimizer.zero_grad()
            output = self.model(data)
            loss = self.loss_fn(output, target)
            loss.backward()

            # è®¡ç®—æ¢¯åº¦èŒƒæ•°ï¼ˆç”¨äºç›‘æ§ï¼‰
            total_norm = self._compute_gradient_norm()

            self.optimizer.step()

            total_loss += loss.item()
            use_time = time.time() - start_time

            # å¢åŠ kwargsä¼ é€’ä¿¡æ¯
            self._log('log_loss', 'train', self.current_epoch, batch_idx,
                     loss.item(),
                     è€—æ—¶=f"{use_time:.2f}s",
                     å­¦ä¹ ç‡=f"{self.optimizer.param_groups[0]['lr']:.2e}",
                     æ¢¯åº¦èŒƒæ•°=f"{total_norm:.4f}",
                     è®¾å¤‡=str(self.device))
            if not self.training:
                break
        return total_loss / len(self.train_loader)

    def _validate(self) -> Dict[str, float]:
        """éªŒè¯é˜¶æ®µ - è®¡ç®—æ‰€æœ‰è¯„ä¼°æŒ‡æ ‡"""
        self.model.eval()
        all_outputs = []
        all_targets = []
        total_val_loss = 0.0

        with torch.no_grad():
            for batch_idx, (data, target) in enumerate(self.val_loader):
                start_time = time.time()
                # åŒæ—¶ç§»åŠ¨dataå’Œtargetåˆ°è®¾å¤‡
                data, target = data.to(self.device), target.to(self.device)
                output = self.model(data)
                all_outputs.append(output.cpu())
                all_targets.append(target.cpu())

                # è®°å½•éªŒè¯æŸå¤±å’Œè€—æ—¶ç­‰ä¿¡æ¯
                val_loss = self.loss_fn(output, target).item()
                total_val_loss += val_loss
                use_time = time.time() - start_time
                self._log('log_loss', 'val', self.current_epoch, batch_idx,
                         val_loss,
                         è€—æ—¶=f"{use_time:.2f}s",
                         æ‰¹æ¬¡å¤§å°=f"{data.size(0)}")

        # è®¡ç®—å¹³å‡éªŒè¯æŸå¤±
        avg_val_loss = total_val_loss / len(self.val_loader)

        # åˆå¹¶è®¡ç®—è¯„ä¼°æŒ‡æ ‡
        combined_output = torch.cat(all_outputs, dim=0)
        combined_target = torch.cat(all_targets, dim=0)

        eval_metrics = {}
        if self.metric_evaluator:
            eval_metrics = self.metric_evaluator.evaluate(combined_output, combined_target)
            # è®°å½•è¯„ä¼°æŒ‡æ ‡
            self._log('log_metrics', self.current_epoch, eval_metrics,
                 éªŒè¯æ ·æœ¬æ•°=f"{combined_output.size(0)}",
                 æœ€ä½³æŒ‡æ ‡=f"{self.best_metric:.4f}")

        # è¿”å›éªŒè¯æŸå¤±å’Œæ‰€æœ‰æŒ‡æ ‡
        eval_metrics['val_loss'] = avg_val_loss
        return eval_metrics

    def _compute_gradient_norm(self) -> float:
        """è®¡ç®—æ¢¯åº¦L2èŒƒæ•°"""
        total_norm = 0.0
        for p in self.model.parameters():
            if p.grad is not None:
                param_norm = p.grad.data.norm(2)  # L2èŒƒæ•°
                total_norm += param_norm.item() ** 2
        return total_norm ** 0.5

    def _should_save_model(self, current_metric: float, eval_metrics: Dict[str, float]) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥ä¿å­˜æ¨¡å‹ - å¥å£®çš„æ¨¡å‹é€‰æ‹©é€»è¾‘"""
        is_improvement = False

        if self.monitor_mode == 'min' and current_metric < self.best_metric:
            is_improvement = True
        elif self.monitor_mode == 'max' and current_metric > self.best_metric:
            is_improvement = True

        # é¢å¤–çš„è´¨é‡æ£€æŸ¥ï¼ˆå¯é€‰ï¼‰
        if is_improvement and self._passes_quality_checks(eval_metrics):
            return True

        return is_improvement

    def _passes_quality_checks(self, eval_metrics: Dict[str, float]) -> bool:
        """è´¨é‡æ£€æŸ¥ - ç¡®ä¿æ¨¡å‹è¾¾åˆ°åŸºæœ¬è´¨é‡æ ‡å‡†"""
        # ç¤ºä¾‹æ£€æŸ¥ï¼šå¦‚æœç›‘æ§æŒ‡æ ‡æ˜¯diceï¼Œç¡®ä¿è‡³å°‘è¾¾åˆ°0.3
        if self.monitor_metric == 'dice' and eval_metrics.get('dice', 0) < 0.3:
            self._log('log_time', f"Diceç³»æ•° {eval_metrics['dice']:.4f} è¿‡ä½ï¼Œè·³è¿‡ä¿å­˜")
            return False

        # ç¤ºä¾‹æ£€æŸ¥ï¼šå¦‚æœç›‘æ§æŒ‡æ ‡æ˜¯hausdorffï¼Œç¡®ä¿ä¸æ˜¯æ— ç©·å¤§
        if self.monitor_metric == 'hausdorff' and eval_metrics.get('hausdorff', float('inf')) == float('inf'):
            self._log('log_time', "Hausdorffè·ç¦»ä¸ºæ— ç©·å¤§ï¼Œè·³è¿‡ä¿å­˜")
            return False

        # ç¤ºä¾‹æ£€æŸ¥ï¼šéªŒè¯æŸå¤±ä¸èƒ½æ˜¯NaN
        if math.isnan(eval_metrics.get('val_loss', 0)):
            self._log('log_time', "éªŒè¯æŸå¤±ä¸ºNaNï¼Œè·³è¿‡ä¿å­˜")
            return False

        return True

    def _save_best_model(self, epoch: int, current_metric: float, eval_metrics: Dict[str, float]):
        """ä¿å­˜æœ€ä½³æ¨¡å‹ - ç»Ÿä¸€çš„æœ€ä½³æ¨¡å‹ä¿å­˜é€»è¾‘"""
        self.best_metric = current_metric
        self.early_stop_counter = 0

        # ä¿å­˜æ¨¡å‹
        self.model_manager.save(
            self.model, self.optimizer, epoch,
            'best_model.pth',
            best_metric=self.best_metric,
            eval_metrics=eval_metrics,
            monitor_metric=self.monitor_metric
        )

        # è®°å½•ä¿å­˜ä¿¡æ¯
        metric_info = f"{self.monitor_metric}: {current_metric:.4f}"
        if self.monitor_metric != 'val_loss':
            metric_info += f" | val_loss: {eval_metrics.get('val_loss', 0):.4f}"

        self._log('log_time', f"ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹ ({metric_info})")

    def _check_early_stop(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦åº”è¯¥æ—©åœ"""
        self.early_stop_counter += 1
        if self.early_stop_counter >= self.patience:
            self._log('log_time', f"ğŸ›‘ æ—©åœè§¦å‘ï¼Œè¿ç»­{self.patience}ä¸ªepochæœªæ”¹å–„")
            return True
        return False

    def _update_learning_rate(self, eval_metrics: Dict[str, float]):
        """æ›´æ–°å­¦ä¹ ç‡ - ä½¿ç”¨éªŒè¯æŸå¤±"""
        val_loss = eval_metrics.get('val_loss', 0)
        if not math.isnan(val_loss) and val_loss != float('inf'):
            self.scheduler.step(val_loss)

            # è®°å½•å­¦ä¹ ç‡å˜åŒ–ï¼ˆå¯é€‰ï¼‰
            current_lr = self.optimizer.param_groups[0]['lr']
            self._log('log_time', f"ğŸ“‰ å­¦ä¹ ç‡æ›´æ–°ä¸º: {current_lr:.2e}")

    def _evaluate_training_progress(self, epoch: int, train_loss: float, eval_metrics: Dict[str, float]) -> Dict[str, Any]:
        """è¯„ä¼°è®­ç»ƒè¿›åº¦ - è¿”å›è®­ç»ƒçŠ¶æ€ä¿¡æ¯"""
        current_metric = eval_metrics.get(self.monitor_metric, eval_metrics.get('val_loss', 0))

        progress_info = {
            'epoch': epoch,
            'train_loss': train_loss,
            'current_metric': current_metric,
            'eval_metrics': eval_metrics,
            'should_save': self._should_save_model(current_metric, eval_metrics),
            'should_stop': False
        }

        # æ£€æŸ¥æ—©åœ
        if not progress_info['should_save']:
            progress_info['should_stop'] = self._check_early_stop()

        return progress_info

    def start_train(self):
        """å¼€å§‹è®­ç»ƒ - ä½¿ç”¨éªŒè¯æŒ‡æ ‡é€‰æ‹©æœ€ä½³æ¨¡å‹"""
        self.train_loader = self.config['train_loader']
        self.val_loader = self.config['val_loader']
        self.training = True

        for epoch in range(self.current_epoch, self.config['epochs'] + 1):
            self.current_epoch = epoch

            # è®°å½•epochå¼€å§‹æ—¶é—´
            epoch_start_time = time.time()

            # è®­ç»ƒé˜¶æ®µ
            train_loss = self._train_epoch()

            # éªŒè¯é˜¶æ®µ
            eval_metrics = self._validate()
            eval_metrics['train_loss'] = train_loss  # ä¹Ÿè®°å½•è®­ç»ƒæŸå¤±

            # æ›´æ–°å­¦ä¹ ç‡
            self._update_learning_rate(eval_metrics)

            # è®°å½•epochæ€»è€—æ—¶
            epoch_use_time = time.time() - epoch_start_time
            self._log('log_time', f"Epoch {epoch} æ€»è€—æ—¶: {epoch_use_time:.2f}s")

            # è¯„ä¼°è®­ç»ƒè¿›åº¦å¹¶æ‰§è¡Œç›¸åº”æ“ä½œ
            progress = self._evaluate_training_progress(epoch, train_loss, eval_metrics)

            if progress['should_save']:
                self._save_best_model(epoch, progress['current_metric'], eval_metrics)

            if progress['should_stop']:
                break

            # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
            if epoch % 10 == 0:
                self.save()

            if not self.training:
                break

        self._log('log_time', "è®­ç»ƒå®Œæˆ!")

    def save(self):
        filename = f'checkpoint_epoch_{self.current_epoch}.pth'
        self.model_manager.save(self.model, self.optimizer, self.current_epoch, filename)
        msg = f'save model to {filename}'
        print(msg)
        return msg

    def stop(self, arg):
        self.save()
        self.training = False
        print("è®­ç»ƒåœæ­¢")
